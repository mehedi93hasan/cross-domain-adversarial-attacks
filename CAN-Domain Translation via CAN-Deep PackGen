import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Input, Reshape
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import json
import matplotlib.pyplot as plt
from tqdm import tqdm

class CANDeepPackGen:
    """
    Implementation of CAN-Deep PackGen algorithm from the paper.
    
    This algorithm transforms adversarial network features into protocol-compliant
    CAN frames that preserve adversarial properties while meeting CAN constraints.
    """
    
    def __init__(self, can_template=None, optimizer_lr=0.001, constraint_weight=0.5):
        """
        Initialize the CAN-Deep PackGen algorithm.
        
        Parameters:
        -----------
        can_template : dict or None, default=None
            Template for mapping network features to CAN fields
        optimizer_lr : float, default=0.001
            Learning rate for the optimizer
        constraint_weight : float, default=0.5
            Weight for constraint loss term (Î» in the paper)
        """
        self.can_template = can_template or self._default_can_template()
        self.optimizer_lr = optimizer_lr
        self.constraint_weight = constraint_weight
        self.model = None
        self.scaler = StandardScaler()
        
    def _default_can_template(self):
        """
        Create a default CAN frame template.
        
        Returns:
        --------
        template : dict
            Default template for mapping network features to CAN fields
        """
        return {
            'arbitration_id': {'range': (0, 0x7FF), 'bits': 11, 'source_range': (0, 11)},
            'data_length_code': {'range': (0, 8), 'bits': 4, 'source_range': (11, 15)},
            'payload': {'range': (0, 255), 'bytes': 8, 'source_range': (16, 80)},
            'timestamp': {'min': 0, 'source_range': (80, 88)},
            'checksum': {'special': 'calculated', 'source_range': None},
        }
    
    def build_model(self, input_dim, output_dim=74):
        """
        Build the transformation model.
        
        Parameters:
        -----------
        input_dim : int
            Number of input features
        output_dim : int, default=74
            Number of output features (for CAN frame)
            
        Returns:
        --------
        model : tf.keras.Model
            The built model
        """
        # Model that transforms network features to CAN frame representation
        inputs = Input(shape=(input_dim,))
        x = Dense(128, activation='relu')(inputs)
        x = Dense(256, activation='relu')(x)
        x = Dense(128, activation='relu')(x)
        outputs = Dense(output_dim, activation='sigmoid')(x)
        
        model = Model(inputs=inputs, outputs=outputs)
        model.compile(optimizer=Adam(self.optimizer_lr), loss='binary_crossentropy')
        
        self.model = model
        return model
    
    def quantize_to_can_fields(self, transformed_values):
        """
        Quantize transformed values to valid CAN frame fields.
        
        Parameters:
        -----------
        transformed_values : array-like
            Values from the transformation model
            
        Returns:
        --------
        can_frame : dict
            CAN frame with valid fields
        """
        can_frame = {}
        
        # Extract values based on template mapping
        for field, config in self.can_template.items():
            source_range = config['source_range']
            
            if source_range is None:
                continue
                
            start, end = source_range
            field_values = transformed_values[start:end]
            
            if field == 'arbitration_id':
                # Convert to 11-bit arbitration ID
                min_val, max_val = config['range']
                arb_id = int(np.sum(field_values * (2 ** np.arange(len(field_values)))))
                arb_id = min(max(arb_id, min_val), max_val)
                can_frame[field] = arb_id
                
            elif field == 'data_length_code':
                # Convert to DLC (0-8)
                min_val, max_val = config['range']
                dlc = int(np.sum(field_values * (2 ** np.arange(len(field_values)))))
                dlc = min(max(dlc, min_val), max_val)
                can_frame[field] = dlc
                
            elif field == 'payload':
                # Convert to payload bytes (8 bytes, each 0-255)
                payload = []
                bytes_count = config['bytes']
                bits_per_byte = 8
                
                for i in range(bytes_count):
                    start_bit = i * bits_per_byte
                    end_bit = start_bit + bits_per_byte
                    byte_bits = field_values[start_bit:end_bit]
                    byte_value = int(np.sum(byte_bits * (2 ** np.arange(len(byte_bits)))))
                    payload.append(min(max(byte_value, 0), 255))
                
                can_frame[field] = payload
                
            elif field == 'timestamp':
                # Convert to timestamp (non-negative)
                timestamp_val = np.sum(field_values * (2 ** np.arange(len(field_values))))
                can_frame[field] = max(timestamp_val, 0)
        
        # Calculate checksum if needed
        if 'checksum' in self.can_template and self.can_template['checksum']['special'] == 'calculated':
            can_frame['checksum'] = self._calculate_checksum(can_frame)
        
        return can_frame
    
    def _calculate_checksum(self, can_frame):
        """
        Calculate CAN frame checksum.
        
        Parameters:
        -----------
        can_frame : dict
            CAN frame fields
            
        Returns:
        --------
        checksum : int
            Calculated checksum
        """
        # Simple XOR checksum implementation (can be replaced with CAN-specific algorithm)
        checksum = 0
        
        # Include arbitration ID in checksum
        checksum ^= can_frame['arbitration_id'] & 0xFF
        checksum ^= (can_frame['arbitration_id'] >> 8) & 0xFF
        
        # Include DLC in checksum
        checksum ^= can_frame['data_length_code'] & 0xFF
        
        # Include payload in checksum
        for byte in can_frame['payload']:
            checksum ^= byte & 0xFF
        
        return checksum & 0xFF  # 8-bit checksum
    
    def validate_timing_constraints(self, can_frames):
        """
        Validate and adjust timing constraints for a sequence of CAN frames.
        
        Parameters:
        -----------
        can_frames : list
            List of CAN frames
            
        Returns:
        --------
        valid_frames : list
            CAN frames with valid timing
        """
        valid_frames = []
        last_timestamp = 0
        
        for frame in can_frames:
            # Ensure minimum inter-frame spacing (10 microseconds)
            min_spacing = 0.01  # milliseconds
            if frame['timestamp'] < last_timestamp + min_spacing:
                frame['timestamp'] = last_timestamp + min_spacing
            
            last_timestamp = frame['timestamp']
            valid_frames.append(frame)
        
        return valid_frames
    
    def can_constraint_loss(self, frames):
        """
        Calculate constraint violation loss for CAN frames.
        
        Parameters:
        -----------
        frames : list
            List of CAN frames
            
        Returns:
        --------
        loss : float
            Constraint violation loss
        """
        loss = 0
        
        for frame in frames:
            # Check arbitration ID constraint
            arb_config = self.can_template['arbitration_id']
            min_val, max_val = arb_config['range']
            if frame['arbitration_id'] < min_val or frame['arbitration_id'] > max_val:
                loss += abs(frame['arbitration_id'] - min(max(frame['arbitration_id'], min_val), max_val))
            
            # Check DLC constraint
            dlc_config = self.can_template['data_length_code']
            min_val, max_val = dlc_config['range']
            if frame['data_length_code'] < min_val or frame['data_length_code'] > max_val:
                loss += abs(frame['data_length_code'] - min(max(frame['data_length_code'], min_val), max_val))
            
            # Check payload constraints
            for byte in frame['payload']:
                if byte < 0 or byte > 255:
                    loss += abs(byte - min(max(byte, 0), 255))
            
            # Check timestamp constraint
            if frame['timestamp'] < 0:
                loss += abs(frame['timestamp'])
        
        return loss
    
    def adversarial_loss(self, frames, ids_model, benign_label=0):
        """
        Calculate adversarial loss (misclassification objective).
        
        Parameters:
        -----------
        frames : list
            List of CAN frames
        ids_model : object
            IDS model to evade
        benign_label : int, default=0
            Label for benign traffic
            
        Returns:
        --------
        loss : float
            Adversarial loss
        """
        # Convert frames to feature representation for IDS model
        features = self.frames_to_features(frames)
        
        # Get prediction probabilities
        probs = ids_model.predict_proba(features)
        
        # Calculate cross-entropy loss for misclassification as benign
        loss = -np.mean(np.log(probs[:, benign_label] + 1e-10))
        
        return loss
    
    def frames_to_features(self, frames):
        """
        Convert CAN frames to feature representation for IDS model.
        
        Parameters:
        -----------
        frames : list
            List of CAN frames
            
        Returns:
        --------
        features : array-like
            Feature representation
        """
        features = []
        
        for frame in frames:
            # Example feature extraction (customize based on your IDS model)
            feature_vector = [
                frame['arbitration_id'],
                frame['data_length_code'],
                *frame['payload'],
                frame['timestamp'],
                frame['checksum']
            ]
            features.append(feature_vector)
        
        return np.array(features)
    
    def optimize_adversarial_frames(self, network_adv_samples, ids_model, iterations=100):
        """
        Optimize CAN frames to maintain adversarial properties while satisfying constraints.
        
        Parameters:
        -----------
        network_adv_samples : array-like
            Adversarial network samples to translate
        ids_model : object
            IDS model to evade
        iterations : int, default=100
            Number of optimization iterations
            
        Returns:
        --------
        optimized_frames : list
            Optimized CAN frames
        """
        # Initial transformation
        frames = []
        for sample in network_adv_samples:
            # Transform network features to CAN frame representation
            transformed = self.model.predict(sample.reshape(1, -1))[0]
            frame = self.quantize_to_can_fields(transformed)
            frames.append(frame)
        
        # Optimize frames
        for _ in tqdm(range(iterations), desc="Optimizing CAN frames"):
            # Calculate total loss
            adv_loss = self.adversarial_loss(frames, ids_model)
            constraint_loss = self.can_constraint_loss(frames)
            total_loss = adv_loss + self.constraint_weight * constraint_loss
            
            # Update frames (simplified gradient-based approach)
            # In a real implementation, you would compute gradients and update more accurately
            frames = self._update_frames(frames, ids_model)
        
        # Final validation of timing constraints
        optimized_frames = self.validate_timing_constraints(frames)
        
        return optimized_frames
    
    def _update_frames(self, frames, ids_model):
        """
        Update frames based on loss gradients (simplified implementation).
        
        Parameters:
        -----------
        frames : list
            List of CAN frames
        ids_model : object
            IDS model to evade
            
        Returns:
        --------
        updated_frames : list
            Updated frames
        """
        # This is a simplified implementation; a full implementation would compute 
        # proper gradients and update more accurately
        updated_frames = []
        
        for frame in frames:
            # Create multiple perturbed versions and select the best one
            candidates = []
            for _ in range(10):
                perturbed = self._perturb_frame(frame.copy())
                candidates.append(perturbed)
            
            # Convert candidates to features
            candidate_features = self.frames_to_features(candidates)
            
            # Get benign probabilities
            probs = ids_model.predict_proba(candidate_features)[:, 0]
            
            # Select the best candidate (highest benign probability)
            best_idx = np.argmax(probs)
            updated_frames.append(candidates[best_idx])
        
        return updated_frames
    
    def _perturb_frame(self, frame):
        """
        Apply small perturbation to CAN frame.
        
        Parameters:
        -----------
        frame : dict
            CAN frame
            
        Returns:
        --------
        perturbed_frame : dict
            Perturbed CAN frame
        """
        # Randomly perturb fields with small changes
        if np.random.random() < 0.2:
            # Perturb arbitration ID
            delta = np.random.randint(-5, 6)
            frame['arbitration_id'] = min(max(frame['arbitration_id'] + delta, 0), 0x7FF)
        
        if np.random.random() < 0.1:
            # Perturb DLC
            delta = np.random.randint(-1, 2)
            frame['data_length_code'] = min(max(frame['data_length_code'] + delta, 0), 8)
        
        # Perturb payload bytes
        for i in range(len(frame['payload'])):
            if np.random.random() < 0.2:
                delta = np.random.randint(-10, 11)
                frame['payload'][i] = min(max(frame['payload'][i] + delta, 0), 255)
        
        # Recalculate checksum
        frame['checksum'] = self._calculate_checksum(frame)
        
        return frame
    
    def translate_network_to_can(self, network_adv_samples, ids_model=None):
        """
        Translate adversarial network samples to CAN frames.
        
        Parameters:
        -----------
        network_adv_samples : array-like
            Adversarial network samples
        ids_model : object or None, default=None
            IDS model for adversarial optimization (optional)
            
        Returns:
        --------
        can_frames : list
            Translated CAN frames
        """
        # Scale the input features if necessary
        scaled_samples = self.scaler.transform(network_adv_samples)
        
        # If model not built yet, build it
        if self.model is None:
            self.build_model(scaled_samples.shape[1])
        
        # Initial transformation
        frames = []
        for sample in scaled_samples:
            # Transform network features to CAN frame representation
            transformed = self.model.predict(sample.reshape(1, -1))[0]
            frame = self.quantize_to_can_fields(transformed)
            frames.append(frame)
        
        # If IDS model provided, optimize the frames
        if ids_model is not None:
            frames = self.optimize_adversarial_frames(scaled_samples, ids_model)
        
        # Final validation of timing constraints
        valid_frames = self.validate_timing_constraints(frames)
        
        return valid_frames
    
    def train(self, network_samples, can_samples, epochs=50, batch_size=32):
        """
        Train the translation model between network and CAN domains.
        
        Parameters:
        -----------
        network_samples : array-like
            Network domain samples
        can_samples : array-like
            CAN domain samples
        epochs : int, default=50
            Number of training epochs
        batch_size : int, default=32
            Batch size for training
            
        Returns:
        --------
        history : dict
            Training history
        """
        print("Training CAN-Deep PackGen model...")
        
        # Scale network samples
        scaled_network = self.scaler.fit_transform(network_samples)
        
        # Process CAN samples to binary format
        processed_can = self._preprocess_can_samples(can_samples)
        
        # Build model if not built yet
        if self.model is None:
            self.build_model(scaled_network.shape[1], processed_can.shape[1])
        
        # Train the model
        history = self.model.fit(
            scaled_network, processed_can,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.2,
            verbose=1
        )
        
        return history
    
    def _preprocess_can_samples(self, can_samples):
        """
        Preprocess CAN samples to binary format.
        
        Parameters:
        -----------
        can_samples : array-like
            CAN samples
            
        Returns:
        --------
        processed : array-like
            Processed binary representation
        """
        # This is a simplified implementation that assumes can_samples are already in a suitable format
        # In a real implementation, this would convert CAN frames to binary representations
        return can_samples
    
    def save_model(self, model_path):
        """
        Save the trained model.
        
        Parameters:
        -----------
        model_path : str
            Path to save the model
            
        Returns:
        --------
        model_path : str
            Path to the saved model
        """
        if self.model is None:
            raise ValueError("Model not trained yet")
        
        # Create directory if doesn't exist
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        # Save the model
        self.model.save(model_path)
        
        # Save the scaler
        scaler_path = os.path.join(os.path.dirname(model_path), "scaler.pkl")
        import joblib
        joblib.dump(self.scaler, scaler_path)
        
        print(f"Model saved to {model_path}")
        print(f"Scaler saved to {scaler_path}")
        
        return model_path
    
    def load_model(self, model_path):
        """
        Load a trained model.
        
        Parameters:
        -----------
        model_path : str
            Path to the saved model
            
        Returns:
        --------
        self : object
            The loaded model
        """
        # Load the model
        self.model = tf.keras.models.load_model(model_path)
        
        # Load the scaler
        scaler_path = os.path.join(os.path.dirname(model_path), "scaler.pkl")
        import joblib
        self.scaler = joblib.load(scaler_path)
        
        print(f"Model loaded from {model_path}")
        print(f"Scaler loaded from {scaler_path}")
        
        return self
    
    def save_frames_to_csv(self, frames, output_file):
        """
        Save CAN frames to CSV file.
        
        Parameters:
        -----------
        frames : list
            List of CAN frames
        output_file : str
            Path to output CSV file
            
        Returns:
        --------
        output_file : str
            Path to the saved file
        """
        # Create directory if needed
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # Convert frames to DataFrame format
        rows = []
        for frame in frames:
            row = {
                'arbitration_id': frame['arbitration_id'],
                'data_length_code': frame['data_length_code'],
                'timestamp': frame['timestamp'],
                'checksum': frame['checksum']
            }
            
            # Add payload bytes
            for i, byte in enumerate(frame['payload']):
                row[f'payload_byte_{i}'] = byte
            
            rows.append(row)
        
        # Create DataFrame and save to CSV
        df = pd.DataFrame(rows)
        df.to_csv(output_file, index=False)
        
        print(f"CAN frames saved to {output_file}")
        
        return output_file


def process_csv_with_can_deep_packgen(network_csv, output_dir='./can_deep_packgen_output', 
                                     train_mode=False, can_csv=None, ids_model=None):
    """
    Process a CSV file with network adversarial examples using CAN-Deep PackGen.
    
    Parameters:
    -----------
    network_csv : str
        Path to CSV file with network adversarial examples
    output_dir : str, default='./can_deep_packgen_output'
        Directory to save outputs
    train_mode : bool, default=False
        Whether to train the model (requires can_csv)
    can_csv : str or None, default=None
        Path to CSV file with CAN samples (required if train_mode=True)
    ids_model : object or None, default=None
        IDS model for adversarial optimization (optional)
        
    Returns:
    --------
    results : dict
        Dictionary with paths to output files
    """
    print(f"Processing network samples from {network_csv}...")
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Load network data
    network_data = pd.read_csv(network_csv)
    
    # Separate features (may need to customize this)
    if 'label' in network_data.columns:
        X = network_data.drop(columns=['label'])
        y = network_data['label']
    else:
        X = network_data
        y = None
    
    # Initialize CAN-Deep PackGen
    can_deep_packgen = CANDeepPackGen()
    
    # Train or load the model
    if train_mode:
        if can_csv is None:
            raise ValueError("CAN samples CSV file required for training mode")
        
        # Load CAN data
        print(f"Loading CAN samples from {can_csv}...")
        can_data = pd.read_csv(can_csv)
        
        # Preprocess CAN data (may need to customize this)
        if 'label' in can_data.columns:
            can_X = can_data.drop(columns=['label'])
        else:
            can_X = can_data
        
        # Train the model
        history = can_deep_packgen.train(X.values, can_X.values, epochs=50, batch_size=32)
        
        # Save the model
        model_path = os.path.join(output_dir, "can_deep_packgen_model")
        can_deep_packgen.save_model(model_path)
        
        # Plot training history
        plt.figure(figsize=(10, 5))
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title('Model Loss')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='upper right')
        plt.savefig(os.path.join(output_dir, "training_history.png"))
    
    else:
        # Try to load existing model
        model_path = os.path.join(output_dir, "can_deep_packgen_model")
        if os.path.exists(model_path):
            print(f"Loading existing model from {model_path}...")
            can_deep_packgen.load_model(model_path)
        else:
            print("No existing model found. Training a simple model...")
            # Simple model training without CAN data (less accurate)
            can_deep_packgen.build_model(X.shape[1])
    
    # Translate network samples to CAN frames
    print("Translating network samples to CAN frames...")
    can_frames = can_deep_packgen.translate_network_to_can(X.values, ids_model)
    
    # Save CAN frames to CSV
    output_file = os.path.join(output_dir, "translated_can_frames.csv")
    can_deep_packgen.save_frames_to_csv(can_frames, output_file)
    
    # Also save as JSON for detailed inspection
    json_output = os.path.join(output_dir, "translated_can_frames.json")
    with open(json_output, 'w') as f:
        json.dump(can_frames[:10], f, indent=4)  # Save first 10 frames for inspection
    
    print(f"\nTranslation complete!")
    print(f"Translated {len(can_frames)} network samples to CAN frames")
    print(f"Results saved to {output_dir}")
    
    return {
        'can_frames_csv': output_file,
        'can_frames_json': json_output,
        'model_path': os.path.join(output_dir, "can_deep_packgen_model") if train_mode else None,
        'training_history': os.path.join(output_dir, "training_history.png") if train_mode else None
    }


# Example usage
if __name__ == "__main__":
    # Replace with your actual file paths
    network_csv = "network_adversarial_examples.csv"
    
    # Process in inference mode (without training)
    results = process_csv_with_can_deep_packgen(
        network_csv=network_csv,
        output_dir="./can_deep_packgen_results",
        train_mode=False
    )
    
    print("\nOutput files:")
    for key, value in results.items():
        if value:
            print(f"  {key}: {value}")
