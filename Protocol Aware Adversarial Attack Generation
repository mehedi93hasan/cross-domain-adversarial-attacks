import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
import os
import json

class ProtocolConstraints:
    """
    Class to define and enforce protocol-specific constraints for different dataset types
    as described in the paper's Protocol-Aware Adversarial Attack Generation section.
    """
    
    def __init__(self, dataset_type='network'):
        """
        Initialize protocol constraints for the specified dataset type.
        
        Parameters:
        -----------
        dataset_type : str, default='network'
            Type of dataset ('network', 'can', 'v2x'). 
            This determines which constraints will be applied.
        """
        self.dataset_type = dataset_type.lower()
        
        # Load default constraints
        self.constraints = self._get_default_constraints()
    
    def _get_default_constraints(self):
        """
        Get default constraints for the specified dataset type.
        
        Returns:
        --------
        constraints : dict
            Dictionary with feature constraints
        """
        if self.dataset_type == 'network':
            # Network traffic constraints (e.g., for KDDCup99, UNSW-NB15)
            return {
                'port': {'min': 0, 'max': 65535},  # Valid port ranges
                'duration': {'min': 0},             # Non-negative duration
                'bytes': {'min': 0},                # Non-negative byte count
                'packets': {'min': 0, 'integer': True},  # Packets must be non-negative integers
                'flags': {'categorical': True, 'values': ['S', 'SF', 'F', 'R', 'P', 'RSTO', 'RSTR', 'RSTOS0', 'OTH']},
                'protocol_type': {'categorical': True, 'values': ['tcp', 'udp', 'icmp']},
                'checksum': {'special': 'checksum'},  # Special handling for checksums
            }
        elif self.dataset_type == 'can':
            # CAN bus constraints (e.g., for CICIoV2024)
            return {
                'Arbitration_ID': {'min': 0, 'max': 0x7FF, 'integer': True},  # 11-bit arbitration ID
                'Data_Length_Code': {'min': 0, 'max': 8, 'integer': True},    # 0-8 bytes
                'Payload_Byte': {'min': 0, 'max': 255, 'integer': True},      # 0-255 for each byte
                'Timestamp_Interval': {'min': 0},                              # Non-negative time
                'Message_Frequency': {'min': 0},                               # Non-negative frequency
                'Signal_Value': {'special': 'signal'},                         # Special handling for signals
                'Bus_Load': {'min': 0, 'max': 100},                            # 0-100% bus load
            }
        else:  # v2x or other
            # Default constraints for V2X (combination of network and CAN)
            return {
                # Network constraints
                'port': {'min': 0, 'max': 65535},
                'duration': {'min': 0},
                'bytes': {'min': 0},
                # CAN constraints
                'Arbitration_ID': {'min': 0, 'max': 0x7FF, 'integer': True},
                'Data_Length_Code': {'min': 0, 'max': 8, 'integer': True},
                'Timestamp_Interval': {'min': 0},
            }
    
    def load_constraints_from_file(self, file_path):
        """
        Load constraints from a JSON file.
        
        Parameters:
        -----------
        file_path : str
            Path to the JSON constraints file
            
        Returns:
        --------
        self : object
        """
        with open(file_path, 'r') as f:
            self.constraints = json.load(f)
        return self
    
    def save_constraints_to_file(self, file_path):
        """
        Save current constraints to a JSON file.
        
        Parameters:
        -----------
        file_path : str
            Path to save the JSON constraints file
            
        Returns:
        --------
        file_path : str
            Path to the saved file
        """
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        with open(file_path, 'w') as f:
            json.dump(self.constraints, f, indent=4)
        
        return file_path
    
    def apply(self, feature_values, feature_names):
        """
        Apply protocol constraints to feature values.
        
        Parameters:
        -----------
        feature_values : array-like
            Feature values to constrain
        feature_names : list
            Names of the features corresponding to the values
            
        Returns:
        --------
        constrained_values : array-like
            Feature values after applying constraints
        """
        constrained_values = feature_values.copy()
        
        for i, feature in enumerate(feature_names):
            # Find matching constraint based on feature name
            constraint = None
            for pattern, const in self.constraints.items():
                if pattern in feature.lower():
                    constraint = const
                    break
            
            if constraint:
                # Apply min/max constraints
                if 'min' in constraint:
                    constrained_values[i] = max(constrained_values[i], constraint['min'])
                
                if 'max' in constraint:
                    constrained_values[i] = min(constrained_values[i], constraint['max'])
                
                # Apply integer constraints
                if constraint.get('integer', False):
                    constrained_values[i] = int(round(constrained_values[i]))
                
                # Special handling for categorical features
                if constraint.get('categorical', False) and 'values' in constraint:
                    # This is simplified; real implementation would handle one-hot encoding
                    pass
                
                # Special handling for checksums and other protocol-specific fields
                if 'special' in constraint:
                    # Placeholder for special handling logic
                    pass
        
        return constrained_values


class VXProjectedGradientDescent:
    """
    Implementation of the V2X-Aware Projected Gradient Descent (VX-PGD) algorithm from the paper.
    
    This class implements Algorithm 2 from the paper, generating adversarial examples
    while respecting protocol-specific constraints.
    """
    
    def __init__(self, model, feature_names, dataset_type='network', epsilon=0.1, alpha=0.01, 
                 iterations=10, protocol_constraints=None):
        """
        Initialize the VX-PGD algorithm.
        
        Parameters:
        -----------
        model : object
            Classification model with predict and predict_proba methods
        feature_names : list
            Names of the features in the dataset
        dataset_type : str, default='network'
            Type of dataset ('network', 'can', 'v2x')
        epsilon : float, default=0.1
            Maximum perturbation (L∞ norm)
        alpha : float, default=0.01
            Step size for gradient updates
        iterations : int, default=10
            Number of iterations
        protocol_constraints : ProtocolConstraints or None, default=None
            Protocol constraints object. If None, create a new one based on dataset_type.
        """
        self.model = model
        self.feature_names = feature_names
        self.epsilon = epsilon
        self.alpha = alpha
        self.iterations = iterations
        
        # Initialize protocol constraints
        if protocol_constraints is None:
            self.protocol_constraints = ProtocolConstraints(dataset_type)
        else:
            self.protocol_constraints = protocol_constraints
    
    def generate(self, X, y, targeted=False, target_class=None):
        """
        Generate adversarial examples using the VX-PGD algorithm.
        
        Parameters:
        -----------
        X : array-like
            Original input samples
        y : array-like
            True labels
        targeted : bool, default=False
            Whether to perform a targeted attack
        target_class : int or None, default=None
            Target class for targeted attacks. If None, select the least-likely class.
            
        Returns:
        --------
        X_adv : array-like
            Adversarial examples
        success_rate : float
            Percentage of successful adversarial examples
        """
        # Ensure inputs are numpy arrays
        X = np.array(X)
        y = np.array(y)
        
        # Initialize adversarial examples with originals
        X_adv = X.copy()
        
        # Convert model to TensorFlow if it's not already
        tf_model = self._convert_to_tf_model(self.model, X.shape[1])
        
        # Track successful attacks
        success = np.zeros(X.shape[0], dtype=bool)
        
        for i in range(X.shape[0]):
            # Skip already successful attacks
            if success[i]:
                continue
            
            x = X[i:i+1]  # Get single sample
            y_true = y[i]
            
            # For targeted attacks, determine target class
            if targeted:
                if target_class is not None:
                    y_target = target_class
                else:
                    # Use least likely class as target
                    probs = self.model.predict_proba(x)[0]
                    y_target = np.argmin(probs)
            else:
                y_target = y_true  # For untargeted attacks, we just use the true class
            
            # Apply VX-PGD algorithm (Algorithm 2 from the paper)
            x_adv = self._vx_pgd(tf_model, x, y_target, targeted)
            
            X_adv[i] = x_adv
            
            # Check if attack was successful
            y_pred = self.model.predict(x_adv.reshape(1, -1))[0]
            success[i] = (y_pred != y_true) if not targeted else (y_pred == y_target)
        
        # Calculate success rate
        success_rate = np.mean(success) * 100
        print(f"Attack success rate: {success_rate:.2f}%")
        
        return X_adv, success_rate
    
    def _vx_pgd(self, model, x, y_target, targeted):
        """
        Core implementation of the VX-PGD algorithm (Algorithm 2).
        
        Parameters:
        -----------
        model : tf.keras.Model
            TensorFlow model for gradient calculation
        x : array-like
            Original input sample
        y_target : int
            Target class
        targeted : bool
            Whether this is a targeted attack
            
        Returns:
        --------
        x_adv : array-like
            Adversarial example
        """
        # Convert to TensorFlow tensor
        x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)
        y_tensor = tf.convert_to_tensor([y_target], dtype=tf.int32)
        
        # Initialize adversarial example
        x_adv = x.copy()
        
        for t in range(self.iterations):
            # Step 5: Compute loss gradient
            with tf.GradientTape() as tape:
                tape.watch(x_tensor)
                logits = model(x_tensor)
                
                if targeted:
                    # For targeted attacks, minimize loss for target class
                    loss = tf.keras.losses.sparse_categorical_crossentropy(y_tensor, logits)
                else:
                    # For untargeted attacks, maximize loss for true class
                    loss = -tf.keras.losses.sparse_categorical_crossentropy(y_tensor, logits)
            
            # Get gradient of loss with respect to input
            gradient = tape.gradient(loss, x_tensor)
            
            # Step 6: Update perturbation using sign of gradient
            gradient_sign = tf.sign(gradient).numpy()
            x_adv = x_adv + self.alpha * gradient_sign[0]
            
            # Step 7: Project to L∞-ball (clip to epsilon range around original)
            x_adv = np.clip(x_adv, x - self.epsilon, x + self.epsilon)
            
            # Step 8: Enforce CAN constraints (Protocol-aware projection)
            x_adv = self.protocol_constraints.apply(x_adv[0], self.feature_names)
            x_adv = x_adv.reshape(1, -1)
            
            # Update tensor for next iteration
            x_tensor = tf.convert_to_tensor(x_adv, dtype=tf.float32)
        
        return x_adv[0]
    
    def _convert_to_tf_model(self, sklearn_model, input_dim):
        """
        Convert a scikit-learn model to a TensorFlow model for gradient computation.
        
        Parameters:
        -----------
        sklearn_model : object
            Scikit-learn model
        input_dim : int
            Input dimension
            
        Returns:
        --------
        tf_model : tf.keras.Model
            TensorFlow model approximating the sklearn model
        """
        # This is a simplified approximation - in practice, you'd need a more accurate conversion
        if hasattr(sklearn_model, 'n_classes_'):
            n_classes = sklearn_model.n_classes_
        else:
            n_classes = len(np.unique(sklearn_model.predict(np.random.random((10, input_dim)))))
        
        # Create a simple neural network as a substitute model
        model = Sequential([
            Dense(64, activation='relu', input_shape=(input_dim,)),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(n_classes, activation='softmax')
        ])
        
        model.compile(optimizer=Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        
        return model


def generate_adversarial_examples(input_file, output_dir='./adversarial_examples', 
                                 target_column='label', dataset_type='network',
                                 epsilon=0.1, alpha=0.01, iterations=10):
    """
    Generate adversarial examples from a dataset using VX-PGD algorithm and save to CSV.
    
    Parameters:
    -----------
    input_file : str
        Path to the input CSV file with selected features
    output_dir : str, default='./adversarial_examples'
        Directory to save output files
    target_column : str, default='label'
        Name of the target column
    dataset_type : str, default='network'
        Type of dataset ('network', 'can', 'v2x')
    epsilon : float, default=0.1
        Maximum perturbation
    alpha : float, default=0.01
        Step size
    iterations : int, default=10
        Number of iterations
        
    Returns:
    --------
    results : dict
        Dictionary with paths to output files
    """
    print(f"Loading data from {input_file}...")
    data = pd.read_csv(input_file)
    
    # Extract features and target
    X = data.drop(columns=[target_column])
    y = data[target_column]
    feature_names = X.columns.tolist()
    
    print(f"Dataset shape: {X.shape}")
    print(f"Number of classes: {len(np.unique(y))}")
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
    
    print("Training a classifier...")
    classifier = RandomForestClassifier(n_estimators=100, random_state=42)
    classifier.fit(X_train, y_train)
    
    # Evaluate classifier
    test_acc = classifier.score(X_test, y_test)
    print(f"Classifier accuracy on test set: {test_acc:.4f}")
    
    # Define protocol constraints
    constraints = ProtocolConstraints(dataset_type)
    
    # Save constraints for reference
    os.makedirs(output_dir, exist_ok=True)
    constraints_file = os.path.join(output_dir, 'protocol_constraints.json')
    constraints.save_constraints_to_file(constraints_file)
    
    print(f"\nGenerating adversarial examples using VX-PGD algorithm...")
    print(f"Parameters: epsilon={epsilon}, alpha={alpha}, iterations={iterations}")
    
    # Initialize VX-PGD algorithm
    vx_pgd = VXProjectedGradientDescent(
        model=classifier,
        feature_names=feature_names,
        dataset_type=dataset_type,
        epsilon=epsilon,
        alpha=alpha,
        iterations=iterations,
        protocol_constraints=constraints
    )
    
    # Generate adversarial examples
    X_adv, success_rate = vx_pgd.generate(X_test.values, y_test.values)
    
    # Create DataFrame with adversarial examples
    X_adv_df = pd.DataFrame(X_adv, columns=feature_names)
    
    # Add true labels and predicted labels
    X_adv_df[target_column] = y_test.values
    X_adv_df['predicted_label'] = classifier.predict(X_adv)
    
    # Add a column indicating whether the attack was successful
    X_adv_df['attack_success'] = X_adv_df[target_column] != X_adv_df['predicted_label']
    
    # Save to CSV
    adv_file = os.path.join(output_dir, 'adversarial_examples.csv')
    X_adv_df.to_csv(adv_file, index=False)
    
    # Save original test data for comparison
    original_file = os.path.join(output_dir, 'original_test_samples.csv')
    test_df = X_test.copy()
    test_df[target_column] = y_test
    test_df.to_csv(original_file, index=False)
    
    # Save perturbation (difference between original and adversarial)
    perturbation = X_adv - X_test.values
    perturbation_df = pd.DataFrame(perturbation, columns=feature_names)
    perturbation_file = os.path.join(output_dir, 'perturbations.csv')
    perturbation_df.to_csv(perturbation_file, index=False)
    
    # Generate summary
    summary = {
        'total_samples': len(X_test),
        'attack_success_rate': success_rate,
        'classifier_accuracy': test_acc,
        'parameters': {
            'epsilon': epsilon,
            'alpha': alpha,
            'iterations': iterations,
            'dataset_type': dataset_type
        }
    }
    
    # Save summary
    summary_file = os.path.join(output_dir, 'attack_summary.json')
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=4)
    
    print(f"\nAdversarial example generation complete!")
    print(f"Results saved to {output_dir}")
    
    return {
        'adversarial_examples': adv_file,
        'original_samples': original_file,
        'perturbations': perturbation_file,
        'summary': summary_file,
        'constraints': constraints_file
    }


# Example usage
if __name__ == "__main__":
    # Replace with the path to your selected features dataset
    input_file = "selected_features_dataset.csv"
    
    # Set parameters
    params = {
        'output_dir': './vxpgd_results',
        'target_column': 'label',  # Replace with your target column name
        'dataset_type': 'network',  # 'network', 'can', or 'v2x'
        'epsilon': 0.1,            # Maximum perturbation
        'alpha': 0.01,             # Step size
        'iterations': 10           # Number of iterations
    }
    
    # Generate adversarial examples
    results = generate_adversarial_examples(input_file, **params)
    
    print("\nOutput files:")
    for key, value in results.items():
        print(f"  {key}: {value}")
